# Integrated LLM
### Introduction
This app helps to use local or cloud LLM from Ollama to diretly read from your computer

### Preparations
1. You need to install Ollama from official site ```curl -fsSL https://ollama.ai/install.sh | sh```

2. If you want use local LLM, you can install from official ```ollama pull <model>```
3. Launch ollama server ```ollama serve```
4. Activate virtual environment ```source .venv/bin/activate```
5. run app ```main.py```

### Using
üìÅ **–§–∞–π–ª–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:**

!read <file> [file2]     - read file(s)

!read folder <path>      - read all files in directory

!analyze <file>          - analyze code from file

!info <file>             - info about file

!search <pattern>        - file search

**üöÄ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ LLM:**

!provider <name>         - change provider for LLM

!model <name>            - change LLM modwel

!models [provider]       - –ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏

!set <provider> <key>   - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å API –∫–ª—é—á

!test [provider]         - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ


### Example
!read main.py

!analyze utils.py

!provider openai
